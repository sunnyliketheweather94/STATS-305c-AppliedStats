{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 15: Gaussian processes",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6430qinE0-Y"
      },
      "source": [
        "# Lecture 15: Gaussian Processes\n",
        "**STATS 305c: Applied Statistics III**\n",
        "\n",
        "Scott Linderman\n",
        "\n",
        "Stanford University\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/slinderman/stats305c/blob/master/notebooks/lecture15_gps.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "Week 8 is about Bayesian nonparametrics and stochastic processes, starting Gaussian processes. This notebook is a demo relating a Bayesian linear regression with RBF features to a squared exponential kernel. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsyo9XScEySL"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh6LXGICHLpM"
      },
      "source": [
        "# Pull the data from HW 1\n",
        "!wget -nc https://raw.githubusercontent.com/slinderman/stats305c/main/assignments/hw1/hw1.pt\n",
        "Xs, ys = torch.load(\"hw1.pt\")\n",
        "\n",
        "# Recall that X is a (N, 2) tensor where the first column is all ones\n",
        "# We just want the x locations (the second column)\n",
        "xs = Xs[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the data\n",
        "plt.plot(xs, ys, 'ko')\n",
        "plt.xlabel(\"$x$\")\n",
        "plt.ylabel(\"$y$\")"
      ],
      "metadata": {
        "id": "PVUgBJMThFro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bNRDZk0FijT"
      },
      "source": [
        "## Linear regression with RBF basis functions\n",
        "\n",
        "Let's start with a Bayesian linear regression using an RBF feature encoding,\n",
        "\\begin{align}\n",
        "\\phi_p(x) &= e^{-\\frac{1}{2\\ell^2} (x - c_p)^2 } \n",
        "\\end{align}\n",
        "with center $c_p$ and width $\\ell$. The model was,\n",
        "\\begin{align}\n",
        "y_n &\\sim \\mathcal{N}\\left( \\sum_p w_p \\phi_p(x_n), \\sigma^2 \\right)\n",
        "\\end{align}\n",
        "where \n",
        "\\begin{align}\n",
        "\\mathbf{w} &\\sim \\mathcal{N}(0, \\lambda I).\n",
        "\\end{align}\n",
        "\n",
        "First, let's look at the basis with different numbers of basis functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XAv16VQFepX"
      },
      "source": [
        "# Set up a finite RBF basis\n",
        "def rbf(x, center, length_scale):\n",
        "    return torch.exp(-0.5 / length_scale**2 * (x - center)**2)\n",
        "\n",
        "def plot_basis(num_basis, xlim=5, length_scale=1.0):\n",
        "    centers = torch.linspace(-xlim, xlim, num_basis)\n",
        "    x_grid = torch.linspace(-xlim, xlim, 500)\n",
        "    for center in centers:\n",
        "        plt.plot(x_grid, rbf(x_grid, center, length_scale))\n",
        "    \n",
        "    plt.xlabel(\"$x$\")\n",
        "    plt.ylabel(\"$y$\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_basis(5)"
      ],
      "metadata": {
        "id": "P7mIEzLUiOEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_basis(10)"
      ],
      "metadata": {
        "id": "ZFGnY23siIim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now fit a linear regression with this basis\n",
        "\n",
        "We will first compute the posterior $p(\\mathbf{w} \\mid \\{x_n, y_n\\}_{n=1}^N)$, then we'll use it to predict $y_{N+1}$ for a grid of points $x_{N+1}$. "
      ],
      "metadata": {
        "id": "mnNDVvcWieFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_rbf_regression(xs, ys, \n",
        "                       xlim=5,\n",
        "                       num_basis=10,\n",
        "                       length_scale=1.0,\n",
        "                       prior_var=1.0,\n",
        "                       lkhd_var=0.5**2):\n",
        "    \"\"\"Fit a Bayesian linear regression with an RBF feature encoding.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    xs, ys: 1D inputs and outputs of the regression\n",
        "    xlim: max absolute value of the RBF centers\n",
        "    num_basis: number of RBF features, evenly spaced between +/-xlim\n",
        "    length_scale: length_scale of the RBF kernels\n",
        "    prior_var: prior variance of the weights\n",
        "    lkhd_var: variance of the likelihood y ~ N(w \\cdot \\phi(x), sigma^2)\n",
        "    \"\"\"\n",
        "    centers = torch.linspace(-xlim, xlim, num_basis)\n",
        "\n",
        "    # Encode the data in the RBFs. \n",
        "    encoded_xs = rbf(xs[:, None], centers[None, :], length_scale)\n",
        "\n",
        "    # Assuming fixed and likelihood variances, compute the posterior \n",
        "    # distribution of the weights\n",
        "    J_post = num_basis / prior_var * torch.eye(num_basis) \\\n",
        "           + encoded_xs.T @ encoded_xs / lkhd_var\n",
        "    h_post = encoded_xs.T @ ys / lkhd_var\n",
        "    Sigma_post = torch.linalg.inv(J_post)\n",
        "    mu_post = Sigma_post @ h_post\n",
        "\n",
        "    # Compute the predictive mean at a dense grid of points\n",
        "    x_grid = torch.linspace(-xlim, xlim, 100)\n",
        "    encoded_grid = rbf(x_grid[:, None], centers[None, :], length_scale)\n",
        "    mu_pred = encoded_grid @ mu_post\n",
        "    Sigma_pred = encoded_grid @ Sigma_post @ encoded_grid.T \\\n",
        "        + lkhd_var * torch.eye(100)\n",
        "\n",
        "    # Plot the mean and +-3 standard deviations of the posterior\n",
        "    for i in range(1, 4):\n",
        "        plt.fill_between(x_grid, \n",
        "                        mu_pred - i * torch.sqrt(torch.diag(Sigma_pred)),\n",
        "                        mu_pred + i * torch.sqrt(torch.diag(Sigma_pred)),\n",
        "                        color='r', alpha=0.1)\n",
        "    plt.plot(x_grid, mu_pred, '-r', lw=3)\n",
        "    plt.plot(xs, ys, 'o')\n",
        "    plt.xlabel(\"$x$\")\n",
        "    plt.xlim(-xlim, xlim)\n",
        "    plt.ylabel(\"$y$\")"
      ],
      "metadata": {
        "id": "QxpKsvqtiPzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjBzbEdLhhXs"
      },
      "source": [
        "fit_rbf_regression(xs, ys, num_basis=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1ANbSBchklO"
      },
      "source": [
        "fit_rbf_regression(xs, ys, num_basis=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vje7vKu3hmun"
      },
      "source": [
        "fit_rbf_regression(xs, ys, num_basis=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ePDWTa7sP9a"
      },
      "source": [
        "### Questions\n",
        "- Why are the mean and variance of the predictions more \"wiggly\" in the first plot?\n",
        "- Why do the predicted means decay back to zero as $|x|$ increases rather than following the linear trend of the data?\n",
        "- What would happen if we increased the prior variance?\n",
        "- What would happen if we increased the likelihood variance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh5f1b5UgWID"
      },
      "source": [
        "## Now use a Gaussian process with a squared exponential kernel\n",
        "\n",
        "In class we introduced the squared exponential kernel \n",
        "\\begin{align}\n",
        "K(x_i, x_j) &= \\varsigma^2 e^{-\\frac{1}{2 \\ell'^2} (x_i - x_j)^2}\n",
        "\\end{align}\n",
        "with variance $\\varsigma^2$ and length scale $\\ell'$. We showed how it arises from Bayesian linear regression with an encoding into an infinite set of radial basis functions.  Concretely, the squared exponential kernel with variance $\\varsigma^2 = \\sqrt{\\pi} \\ell \\lambda$ and length scale $\\ell' = \\sqrt{2} \\ell$ is the infinite limit of the RBF regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msgD76fxfHpC"
      },
      "source": [
        "def fit_sqexp_gp(xs, ys, \n",
        "                 length_scale=1.0,\n",
        "                 prior_var=1.0,\n",
        "                 lkhd_var=0.5**2,\n",
        "                 xlim=5.0):\n",
        "    N = len(xs)\n",
        "    \n",
        "    # Define the squared exponential kernel function. Account for the scaling\n",
        "    # constants necessary to equate with the RBF regression\n",
        "    def kernel(x_i, x_j): \n",
        "        scale = 0.1 * torch.sqrt(torch.tensor(torch.pi)) * length_scale * prior_var\n",
        "        return scale * torch.exp(-0.5 / (torch.sqrt(torch.tensor(2)) * \\\n",
        "                                         length_scale)**2 * (x_i - x_j)**2)\n",
        "\n",
        "    # Compute the Gram matrix\n",
        "    K = kernel(xs[:, None], xs[None, :])\n",
        "\n",
        "    # Compute the predictive mean at a dense grid of points\n",
        "    x_grid = torch.linspace(-xlim, xlim, 1000)\n",
        "    K_grid = kernel(xs[:, None], x_grid[None, :]) \n",
        "\n",
        "    K_chol = torch.linalg.cholesky(K + lkhd_var * torch.eye(N))\n",
        "    mu_pred = K_grid.T @ torch.cholesky_solve(ys[:, None], K_chol)[:, 0]\n",
        "    Sigma_pred = kernel(x_grid[:, None], x_grid[None, :]) \\\n",
        "        - K_grid.T @ torch.cholesky_solve(K_grid, K_chol) + lkhd_var * torch.eye(1000)\n",
        "    \n",
        "    # Plot the mean and +-3 standard deviations of the posterior\n",
        "    for i in range(1, 4):\n",
        "        plt.fill_between(x_grid, \n",
        "                         mu_pred - i * torch.sqrt(torch.diag(Sigma_pred)),\n",
        "                         mu_pred + i * torch.sqrt(torch.diag(Sigma_pred)),\n",
        "                         color='r', alpha=0.1)\n",
        "    plt.plot(x_grid, mu_pred, '-r', lw=3)\n",
        "    plt.plot(xs, ys, 'o')\n",
        "    plt.xlabel(\"$x$\")\n",
        "    plt.xlim(-xlim, xlim)\n",
        "    plt.ylabel(\"$y$\")\n",
        "\n",
        "fit_sqexp_gp(xs, ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTkaSBp6tDre"
      },
      "source": [
        "### Comparison to the RBF regression \n",
        "\n",
        "This should look like the RBF regression as we take the number of basis functions to infinity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq3pcSWmpp5s"
      },
      "source": [
        "fit_rbf_regression(xs, ys, num_basis=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTgGmLNF-DqG"
      },
      "source": [
        "### Questions\n",
        "- What happens if you increase the prior variance?\n",
        "- What happens if you _decrease_ the likelihood variance?\n",
        "- What happens if you _decrease_ the length scale?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GP Classification with a Probit Mean Function"
      ],
      "metadata": {
        "id": "6wkh_hYPxfux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's visualize the joint distribution over $f(x)$ and $z$, assuming\n",
        "\\begin{align}\n",
        "f(x) &\\sim \\mathcal{N}(0, 1) \\\\\n",
        "z &\\sim \\mathcal{N}(f(x), 1) \\cdot \\mathbb{I}[z > 0]\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "POp1_SlI0aYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a g\n",
        "xlim = 5\n",
        "x_grid = torch.linspace(-xlim, xlim, 50)\n",
        "z_grid = torch.linspace(0, xlim, 50)\n",
        "XX, ZZ = torch.meshgrid(x_grid, z_grid)\n",
        "\n",
        "# Compute the log joint probability\n",
        "lp_xz = torch.distributions.Normal(0, 1).log_prob(XX)\n",
        "lp_xz += torch.distributions.Normal(XX, 1).log_prob(ZZ)\n",
        "lp_xz -= torch.logsumexp(lp_xz, (0, 1))\n",
        "# p_xz = torch.exp(lp_xz)\n",
        "\n",
        "# Compute the marginal\n",
        "lp_x = torch.logsumexp(lp_xz, axis=1)\n",
        "lp_x -= (torch.logsumexp(lp_x, 0) + torch.log(x_grid[1] - x_grid[0]))\n",
        "lp_x_prior = torch.distributions.Normal(0, 1).log_prob(x_grid)\n",
        "lp_x_lkhd = torch.distributions.Normal(0, 1).cdf(x_grid)\n",
        "\n",
        "from matplotlib.gridspec import GridSpec\n",
        "gs = GridSpec(2, 1, height_ratios=(2, 1))\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "ax1.contour(XX, ZZ, lp_xz, 50)\n",
        "ax1.set_ylabel(\"$z$\")\n",
        "\n",
        "ax2 = fig.add_subplot(gs[1, 0], sharex=ax1)\n",
        "ax2.plot(x_grid, torch.exp(lp_x_prior), '-r', label=\"prior\")\n",
        "ax2.plot(x_grid, lp_x_lkhd, '-k', label=\"lkhd\")\n",
        "ax2.plot(x_grid, torch.exp(lp_x), lw=3, label=\"posterior\")\n",
        "ax2.set_xlabel(\"$f(x)$\")\n",
        "ax2.set_ylabel(\"$p(f(x))$\")\n",
        "ax2.legend(loc=\"upper left\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"probit.pdf\")"
      ],
      "metadata": {
        "id": "9oj9d5Jaxi-g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}