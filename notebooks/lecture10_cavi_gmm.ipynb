{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoYdLutn7dUa"
      },
      "source": [
        "# Coordinate Ascent Variational Inference for Gaussian Mixture Models\n",
        "\n",
        "\n",
        "**STATS 305C: Applied Statistics III**\n",
        "\n",
        "_Stanford University. Spring, 2022._\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/slinderman/stats305c/blob/master/notebooks/lecture10_cavi_gmm.ipynb)\n",
        "\n",
        "In this notebook you'll practice deriving and implementing coordinate ascent variational inference (CAVI) for a model you now know and love, the Gaussian mixture model. We will focus on the simple case in which the covariances are known for each mixture component, but we'll give some pointers at the end as to how you could generalize this approach.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Generative model\n",
        "Assume each mixture component has identity covariance. Then the generative model is,\n",
        "\n",
        "1. Sample parameters for each mixture component:\n",
        "\\begin{align}\n",
        "\\theta_k &\\sim \\mathcal{N}(0, \\beta^{-1} I) \\\\\n",
        "\\end{align}\n",
        "2. Sample mixture weights,\n",
        "\\begin{align}\n",
        "\\pi &\\sim \\mathrm{Dir}(\\alpha) \n",
        "\\end{align}\n",
        "3. Sample mixture assignments for each data point,\n",
        "\\begin{align}\n",
        "z_n &\\sim \\mathrm{Cat}(\\pi)\n",
        "\\end{align}\n",
        "4. Sample data points given parameters and assignments,\n",
        "\\begin{align}\n",
        "x_n &\\sim \\mathcal{N}(\\theta_{z_n}, I)\n",
        "\\end{align}\n",
        "\n",
        "As we showed in class, you can write the log joint probability as,\n",
        "\\begin{align}\n",
        "\\log p(X, Z, \\Theta, \\pi) &= \\sum_{n=1}^N \\sum_{k=1}^K \\left[ \\mathbb{I}[z_n = k] \\left(\\log \\mathcal{N}(x_n \\mid \\theta_k, I)  + \\log \\pi_k \\right)  \\right] + \\sum_{k=1}^K [\\log \\mathcal{N}(\\theta_k \\mid 0, \\beta^{-1} I)] + \\log \\mathrm{Dir}(\\pi \\mid \\alpha) \n",
        "\\end{align}\n",
        "where we have used the shorthand $X = \\{x_n\\}_{n=1}^N$, $Z = \\{z_n\\}_{n=1}^N$, and $\\Theta = \\{\\theta_k\\}_{k=1}^K$. \n"
      ],
      "metadata": {
        "id": "dnZAdL78SSU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Derive the conditional distributions\n",
        "\n",
        "To speed you along, we've given you the answers for this part. Double check that you understand how we arrived at them, then proceed to Part 1 where you'll derive the corresponding CAVI updates."
      ],
      "metadata": {
        "id": "9Puzugc0VF3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1a: Conditional distribution of the mixture means\n",
        "Derive $p(z_n \\mid x_n, \\pi, \\{\\theta_k\\}_{k=1}^K)$"
      ],
      "metadata": {
        "id": "tteo0VyFVuiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Solution:** \n",
        "\\begin{align}\n",
        "p(z_n \\mid x_n, \\pi, \\{\\theta_k\\}_{k=1}^K) \n",
        "&\\propto p(z_n \\mid \\pi) p(x_n \\mid \\Theta, z_n) \\\\\n",
        "&\\propto \\pi_{z_n} \\mathcal{N}(x_n \\mid \\theta_{z_n}, I)\n",
        "\\end{align}\n",
        "\n",
        "Normalizing,\n",
        "\\begin{align}\n",
        "p(z_n = k \\mid x_n, \\pi, \\{\\theta_k\\}_{k=1}^K) \n",
        "&= \\frac{\\pi_{k} \\mathcal{N}(x_n \\mid \\theta_{k}, I)}{\\sum_{j=1}^K \\pi_{j} \\mathcal{N}(x_n \\mid \\theta_{j}, I)}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "P14Wfh4fVqSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Problem 1b: Conditional distribution of the mixture weights\n",
        "Derive $p(\\pi \\mid \\{z_n\\}_{n=1}^N, \\alpha)$\n"
      ],
      "metadata": {
        "id": "1k_seUIUVoOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Solution:** \n",
        "\\begin{align}\n",
        "p(\\pi \\mid \\{z_n\\}_{n=1}^N, \\alpha) \n",
        "&\\propto p(\\pi \\mid \\alpha) \\prod_{n=1}^N p(z_n \\mid \\pi) \\\\\n",
        "&\\propto \\mathrm{Dir}(\\pi \\mid \\alpha) \\prod_{n=1}^N \\prod_{k=1}^K \\pi_k^{\\mathbb{I}[z_n = k]} \\\\\n",
        "&= \\mathrm{Dir}(\\pi \\mid [\\alpha_1 + N_1, \\ldots, \\alpha_K + N_K])\n",
        "\\end{align}\n",
        "where\n",
        "\\begin{align}\n",
        " N_k = \\sum_{n=1}^N \\mathbb{I}[z_n = k]\n",
        "\\end{align}\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "F27G4p54VwMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1c: Conditional distribution of the mixture means\n",
        "Derive $p(\\theta_k \\mid \\{x_n, z_n\\}_{n=1}^N, 0, \\beta^{-1} I)$\n"
      ],
      "metadata": {
        "id": "4yQLyN7DVhOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Solution:** \n",
        "\\begin{align}\n",
        "p(\\theta_k \\mid \\{x_n, z_n\\}_{n=1}^N, 0, \\beta) \n",
        "&\\propto p(\\theta_k \\mid \\beta) \\prod_{n=1}^N p(x_n \\mid \\theta_k)^{\\mathbb{I}[z_n = k]} \\\\\n",
        "&\\propto \\mathcal{N}(\\theta_k \\mid 0, \\beta^{-1} I) \\prod_{n=1}^N \\mathcal{N}(x_n \\mid \\theta_k, I)^{\\mathbb{I}[z_n = k]} \\\\\n",
        "&\\propto \\exp \\left\\{-\\frac{\\beta + N_k}{2} \\theta_k^\\top  \\theta_k + \\theta_k^\\top \\left( \\sum_{n=1}^N \\mathbb{I}[z_n = k] x_n \\right) \\right\\} \\\\\n",
        "&\\propto \\mathcal{N}\\left(\\theta_k \\, \\bigg| \\, \\frac{\\bar{x}_k}{\\beta + N_k}, \\, \\frac{1}{\\beta + N_k} I  \\right)\n",
        "\\end{align}\n",
        "where\n",
        "\\begin{align}\n",
        "\\bar{x}_k &= \\sum_{n=1}^N \\mathbb{I}[z_n = k] x_n \\\\\n",
        "N_k &= \\sum_{n=1}^N \\mathbb{I}[z_n = k]\n",
        "\\end{align}\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "DUX9b4W8VzFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Derive the CAVI updates\n",
        "For the Gaussian mixture model we will use a mean field posterior approximation, which assumes that the parameters and latent variables are all independent:\n",
        "\\begin{align}\n",
        "q(Z, \\Theta, \\pi) &= \\prod_{n=1}^N \\left[q(z_n; \\lambda_n^{(z)}) \\right] \\, \\prod_{k=1}^K \\left[q(\\theta_k; \\lambda_k^{(\\theta)}) \\right] \\, q(\\pi; \\lambda^{(\\pi)})\n",
        "\\end{align}\n",
        "\n",
        "We will find the optimal variational approximation via coordinate ascent on the ELBO. Recall that the general form for a CAVI update is to set the mean field factor for one variable $q(\\vartheta_j; \\lambda_j)$ equal to,\n",
        "\\begin{align}\n",
        "q(\\vartheta_j; \\lambda_j) \\propto \\exp \\left\\{\\mathbb{E}_{q(\\vartheta_{\\neg j}; \\lambda_{\\neg j})}\\left[ \\log p(\\vartheta_j \\mid x, \\vartheta_{\\neg j}) \\right] \\right\\}\n",
        "\\end{align}\n",
        "or equivalently, \n",
        "\\begin{align}\n",
        "\\log q(\\vartheta_j; \\lambda_j) &= \\mathbb{E}_{q(\\vartheta_{\\neg j}; \\lambda_{\\neg j})}\\left[ \\log p(\\vartheta_j \\mid x, \\vartheta_{\\neg j}) \\right] + c\n",
        "\\end{align}\n",
        "For models like this one, which are built of exponential family distributions with conjugate priors, these CAVI updates will have simple closed form solutions.\n",
        "\n",
        "In Problem 1, you already derived the conditional distributions. Now you just have to compute the expected log conditional densities, where the expectation is taken with respect to the other variables."
      ],
      "metadata": {
        "id": "-D8TFnu0V2Td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2a: Derive the CAVI update for the mixture means\n",
        "\n",
        "The mixture assignments are discrete variables $z_n \\in \\{1,\\ldots,K\\}$ so their variational posterior must be a discrete distribution; aka a categorical distribution:\n",
        "\\begin{align}\n",
        "q(z_n; \\lambda_n^{(z)}) &= \\mathrm{Cat}(z_n ; \\lambda_n^{(z)})\n",
        "\\end{align}\n",
        "where \n",
        "\\begin{align}\n",
        "\\lambda_n^{(z)} = \\left[ \\lambda_{n,1}^{(z)}, \\ldots, \\lambda_{n,K}^{(z)} \\right]^\\top\n",
        "\\end{align}\n",
        "are the variational parameters. They must be non-negative and sum to one. These are equivalent to the _responsibilities_ from Week 4. \n",
        "\n",
        "Derive an expression for $\\lambda_{n,k}^{(z)}$ in terms of $\\mathbb{E}_{q(\\pi)}[\\log \\pi_k]$ and $\\mathbb{E}_{q(\\theta_k)}[\\log \\mathcal{N}(x_n \\mid \\theta_k, I)]$"
      ],
      "metadata": {
        "id": "tKFWYyjBWp3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rUG0SNB5aN5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2b: Derive the CAVI update for the mixture weights\n",
        "\n",
        "Show that \n",
        "\\begin{align}\n",
        "q(\\pi; \\lambda^{(\\pi)}) = \\mathrm{Dir}(\\pi ; \\lambda^{(\\pi)})\n",
        "\\end{align}\n",
        "where $\\lambda^{(\\pi)} \\in \\mathbb{R}_+^{K}$ is a vector of posterior concentrations. \n",
        "\n",
        "Derive the optimal update for the variational parameters $\\lambda^{(\\pi)}$ in terms of $\\alpha$ and $\\lambda_{n,k}^{(z)}$, using the fact that\n",
        "\\begin{align}\n",
        "\\mathbb{E}_{q(z_n)}[\\mathbb{I}[z_n = k]] &= \\lambda_{n,k}^{(z)}.\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "4K_Ltv1UYjx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JD0Rj1ScaO6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2c: Derive the CAVI updates for the mixture means\n",
        "\n",
        "Show that the optimal $q(\\theta_k; \\lambda_k^{(\\theta)})$ is a Gaussian distribution,\n",
        "\\begin{align}\n",
        "q(\\theta_k; \\lambda_k^{(\\theta)}) = \\mathcal{N}(\\theta_k; \\mu_k, \\Sigma_k)\n",
        "\\end{align}\n",
        "with $\\lambda_k^{(\\theta)} = ({\\mu}_k, {\\Sigma}_k)$.\n",
        "\n",
        "Derive the variational parameter updates in terms of,\n",
        "\\begin{align}\n",
        " \\bar{x}_k &\\triangleq \\sum_{n=1}^N \\mathbb{E}_{q(z_n)} \\left[ \\mathbb{I}[z_n = k] \\cdot x_n \\right] \\\\\n",
        " &= \\sum_{n=1}^N \\lambda_{n,k}^{(z)} x_n\n",
        "\\end{align}\n",
        "and\n",
        "\\begin{align}\n",
        "N_k &\\triangleq \\sum_{n=1}^N \\mathbb{E}_{q(z_n)} \\left[ \\mathbb{I}[z_n = k] \\right] \\\\\n",
        "&= \\sum_{n=1}^N \\lambda_{n,k}^{(z)}.\n",
        "\\end{align}\n",
        "as well as the prior parameter $\\beta$."
      ],
      "metadata": {
        "id": "A7LYEfl9ZXn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jkigwffZaR4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2d: Derive the Gaussian cross entropy\n",
        "\n",
        "The _negative cross entropy_ between $q(x)$ and $p(x)$ is defined as,\n",
        "\\begin{align}\n",
        "\\mathbb{E}_{q(x)}[\\log p(x)]\n",
        "\\end{align}\n",
        "\n",
        "Since $q(\\theta_k)$ is Gaussian, and since $\\mathcal{N}(x_n \\mid \\theta_k, I) = \\mathcal{N}(\\theta_k \\mid x_n, I)$, we now recognize the $\\mathbb{E}_{q(\\theta_k)}[\\log \\mathcal{N}(x_n \\mid \\theta_k, I)]$ term in our $q(z)$ update as the negative cross entropy between two multivariate normal distributions.\n",
        "\n",
        "Show that the negative cross entropy between two multivariate normal distributions is,\n",
        "\\begin{align}\n",
        "\\mathbb{E}_{\\mathcal{N}(x \\mid \\mu_1, \\Sigma_1)}[\\log \\mathcal{N}(x \\mid \\mu_2, \\Sigma_2)] \n",
        "&= \n",
        "\\log \\mathcal{N}(\\mu_1 \\mid \\mu_2, \\Sigma_2) -\\tfrac{1}{2} \\mathrm{Tr}(\\Sigma_1 \\Sigma_2^{-1})\n",
        "\\end{align}\n"
      ],
      "metadata": {
        "id": "QxmWxu1qC2J-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SkFifNeUTSm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Derive the ELBO\n"
      ],
      "metadata": {
        "id": "ICOLiU3la34S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 3a: Break the ELBO into parts\n",
        "The ELBO is generically written as,\n",
        "\\begin{align}\n",
        "\\mathcal{L}(\\lambda) &=\n",
        "\\mathbb{E}_q [\\log p(X, Z, \\Theta, \\pi) - \\log q(Z, \\Theta, \\pi) ]\\end{align}\n",
        "Rewrite the ELBO in terms of the following quantities,\n",
        "- $\\mathbb{E}_{q(z_n)}[\\mathbb{I}[z_n = k]] \\, \\mathbb{E}_{q(\\theta_k)}[\\log \\mathcal{N}(x_n \\mid \\theta_k, I)]$\n",
        "- $\\mathbb{E}_{q(\\pi)} [\\mathrm{KL}(q(z_n) \\, \\| \\, \\pi)]$\n",
        "- $\\mathrm{KL}(q(\\pi) \\, \\| \\, p(\\pi))$\n",
        "- $\\mathrm{KL}(q(\\theta_k) \\, \\| \\, p(\\theta_k))$"
      ],
      "metadata": {
        "id": "vcteW3JL8lHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fwip3yxe9GKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 3b: Derive closed form expressions for each term in the ELBO\n",
        "\n",
        "Find closed form expressions for each term from Problem 3a."
      ],
      "metadata": {
        "id": "K_SB70Km8wpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "_Your answer here_\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "uy9dZ_nD9G5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4: Implementation"
      ],
      "metadata": {
        "id": "D-gCR46b-TPn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFN1-wvIue3K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.distributions import Dirichlet, MultivariateNormal, Categorical\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Ellipse\n",
        "import matplotlib.transforms as transforms\n",
        "\n",
        "from tqdm.auto import trange"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper function to draw ellipse\n",
        "def confidence_ellipse(mean, cov, ax, n_std=3.0, facecolor='none', **kwargs):\n",
        "    \"\"\"\n",
        "    Modified from: https://matplotlib.org/3.5.0/gallery/\\\n",
        "        statistics/confidence_ellipse.html\n",
        "    Create a plot of the covariance confidence ellipse of *x* and *y*.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mean: vector-like, shape (n,)\n",
        "        Mean vector.\n",
        "        \n",
        "    cov : matrix-like, shape (n, n)\n",
        "        Covariance matrix.\n",
        "\n",
        "    ax : matplotlib.axes.Axes\n",
        "        The axes object to draw the ellipse into.\n",
        "\n",
        "    n_std : float\n",
        "        The number of standard deviations to determine the ellipse's radiuses.\n",
        "\n",
        "    **kwargs\n",
        "        Forwarded to `~matplotlib.patches.Ellipse`\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    matplotlib.patches.Ellipse\n",
        "    \"\"\"\n",
        "    # compute the 2D covariance ellipse\n",
        "    pearson = cov[0, 1] / torch.sqrt(cov[0, 0] * cov[1, 1])\n",
        "    ell_radius_x = torch.sqrt(1 + pearson)\n",
        "    ell_radius_y = torch.sqrt(1 - pearson)\n",
        "    ellipse = Ellipse((0, 0), \n",
        "                      width=ell_radius_x * 2, \n",
        "                      height=ell_radius_y * 2,\n",
        "                      facecolor=facecolor, \n",
        "                      **kwargs)\n",
        "\n",
        "    # Calculating the standard deviation\n",
        "    # the square root of the variance and multiplying\n",
        "    # with the given number of standard deviations.\n",
        "    scale = torch.sqrt(torch.diag(cov) * n_std)\n",
        "    \n",
        "    # Transform the ellipse by rotating, scaling, and translating\n",
        "    transf = transforms.Affine2D() \\\n",
        "        .rotate_deg(45) \\\n",
        "        .scale(*scale) \\\n",
        "        .translate(*mean)\n",
        "    ellipse.set_transform(transf + ax.transData)\n",
        "\n",
        "    # Add the patch to the axis\n",
        "    return ax.add_patch(ellipse)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WdCUApSU9VvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write some helper fucntions for KL divergences and cross entropies\n"
      ],
      "metadata": {
        "id": "7Nyg-s9Yx1JN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoqrOOfvue3N"
      },
      "outputs": [],
      "source": [
        "def dirichlet_expected_log(dirichlet):\n",
        "    \"\"\"Helper function to compute expected log under Dirichlet distribution.\n",
        "\n",
        "    Args:\n",
        "        dirichlet: A torch.distributions.Dirichlet object with a batch shape of\n",
        "            (...,) and a event shape of (K,).\n",
        "\n",
        "    Returns:\n",
        "        (...,K) tensor of expected logs, E[\\log \\pi], under the Dirichlet.\n",
        "    \"\"\"\n",
        "    concentration = dirichlet.concentration\n",
        "    return torch.special.digamma(concentration) - \\\n",
        "           torch.special.digamma(concentration.sum(dim=-1, keepdims=True))\n",
        "\n",
        "\n",
        "def dirichlet_log_normalizer(concentration):\n",
        "    \"\"\"Compute the log normalizing constant of a Dirichlet distribution with\n",
        "    the specificed concentration.\n",
        "\n",
        "    Args:\n",
        "        concentration: (...,K) tensor of concentration parameters\n",
        "\n",
        "    Returns:\n",
        "        (...,) batch of log normalizers\n",
        "    \"\"\"\n",
        "    return torch.special.gammaln(concentration).sum(dim=-1) - \\\n",
        "        torch.special.gammaln(concentration.sum(dim=-1))\n",
        "\n",
        "def dirichlet_kl(q, p):\n",
        "    \"\"\"Compute the KL divergence between two Dirichlet disdtributions\n",
        "\n",
        "    Args:\n",
        "        q: A torch.distributions.Dirichlet object\n",
        "        p: A torch.distributions.Dirichlet object over the same domain\n",
        "\n",
        "    Returns:\n",
        "        A (batch of) KL divergence(s) between q and p.\n",
        "    \"\"\"\n",
        "    kl = -dirichlet_log_normalizer(q.concentration)\n",
        "    kl += dirichlet_log_normalizer(p.concentration)\n",
        "    kl += torch.sum((q.concentration - p.concentration) * \\\n",
        "                    dirichlet_expected_log(q), dim=-1)\n",
        "    return kl\n",
        "\n",
        "\n",
        "def gaussian_neg_cross_entropy(q, p):\n",
        "    \"\"\"Compute the negative cross entropy between two Gaussian distributions,\n",
        "        \n",
        "        -E_{q(x | \\mu_q, \\Sigma_q)}[\\log p(x | \\mu_p, \\Sigma_p)]\n",
        "\n",
        "    Args:\n",
        "        q: A torch.distributions.MultivariateNormal object\n",
        "        p: A torch.distributions.MultivariateNormal object\n",
        "\n",
        "    Returns:\n",
        "        A (batch of) cross entropy(ies) between q and p.\n",
        "    \"\"\"\n",
        "    mu_q = q.mean\n",
        "    Sigma_q = q.covariance_matrix\n",
        "    mu_p = p.mean\n",
        "    Sigma_p = p.covariance_matrix\n",
        "\n",
        "    # Compute the multivariate normal cross entropy\n",
        "    return p.log_prob(mu_q) - 0.5 * torch.diagonal(\n",
        "        torch.linalg.solve(Sigma_p, Sigma_q), dim1=-2, dim2=-1).sum(dim=-1)\n",
        "\n",
        "\n",
        "def gaussian_kl(q, p):\n",
        "    \"\"\"Compute the KL divergence between two Gaussian distributions,\n",
        "        \n",
        "        E_{q(x | \\mu_q, \\Sigma_q)}[\\log q(x | \\mu_q, \\Sigma_q) \n",
        "                                  - \\log p(x | \\mu_p, \\Sigma_p)]\n",
        "\n",
        "    Notes:\n",
        "        This function is not at all optimized for speed. A better implementation\n",
        "        would use the Cholesky of the covariance matrix rather than explicitly\n",
        "        inverting it. C.f. https://github.com/tensorflow/probability/blob/main/\n",
        "        tensorflow_probability/python/distributions/mvn_linear_operator.py#L327\n",
        "\n",
        "    Args:\n",
        "        q: A torch.distributions.MultivariateNormal object\n",
        "        p: A torch.distributions.MultivariateNormal object\n",
        "\n",
        "    Returns:\n",
        "        A (batch of) cross entropy(ies) between q and p.\n",
        "    \"\"\"\n",
        "    mu_q = q.mean\n",
        "    Sigma_q = q.covariance_matrix\n",
        "    mu_p = p.mean\n",
        "    Sigma_p = p.covariance_matrix\n",
        "    Sigma_p_inv = torch.inverse(Sigma_p)\n",
        "    dim = mu_q.shape[-1]\n",
        "\n",
        "    kl = 0.5 * (torch.logdet(Sigma_p) - torch.logdet(Sigma_q))\n",
        "    kl -= 0.5 * dim\n",
        "    kl += 0.5 * torch.diagonal(\n",
        "        Sigma_p_inv @ Sigma_q, dim1=-2, dim2=-1).sum(dim=-1)\n",
        "    kl += 0.5 * torch.einsum('...i,...ij,...j->...', \n",
        "                             mu_p - mu_q, Sigma_p_inv, mu_p - mu_q)\n",
        "    return kl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# These functions should broadcast appropriately\n",
        "p = torch.distributions.MultivariateNormal(torch.zeros(5, 1, 2), torch.eye(2))\n",
        "q = torch.distributions.MultivariateNormal(torch.zeros((1, 10, 2)), \n",
        "                                           2 * torch.eye(2).repeat(1, 10, 1, 1))\n",
        "out = gaussian_kl(q, p)\n",
        "out"
      ],
      "metadata": {
        "id": "NwoQx6qKWfih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test our implementations against tensorflow probability\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from tensorflow_probability.substrates import jax as tfp\n",
        "tfd = tfp.distributions\n",
        "\n",
        "# Code to suppress stupid TFP warnings\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "class CheckTypesFilter(logging.Filter):\n",
        "    def filter(self, record):\n",
        "        return \"check_types\" not in record.getMessage()\n",
        "logger.addFilter(CheckTypesFilter())\n",
        "\n",
        "# Test Gaussian KL\n",
        "p = MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
        "q = MultivariateNormal(torch.ones(2), 2 * torch.eye(2))\n",
        "print(\"my kl:  \", gaussian_kl(q, p).numpy())\n",
        "p2 = tfd.MultivariateNormalFullCovariance(jnp.zeros(2), jnp.eye(2))\n",
        "q2 = tfd.MultivariateNormalFullCovariance(jnp.ones(2), 2 * jnp.eye(2))\n",
        "print(\"tfp kl: \", q2.kl_divergence(p2))\n",
        "\n",
        "# Test Gaussian cross entropy\n",
        "p = MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
        "q = MultivariateNormal(torch.ones(2), 2 * torch.eye(2))\n",
        "print(\"my ce:  \", gaussian_neg_cross_entropy(q, p).numpy())\n",
        "p2 = tfd.MultivariateNormalFullCovariance(jnp.zeros(2), jnp.eye(2))\n",
        "q2 = tfd.MultivariateNormalFullCovariance(jnp.ones(2), 2 * jnp.eye(2))\n",
        "print(\"tfp ce: \", -q2.cross_entropy(p2))\n",
        "\n",
        "# Test Dirichlet KL\n",
        "p = Dirichlet(torch.ones(2))\n",
        "q = Dirichlet(2 * torch.ones(2))\n",
        "print(\"my kl:  \", dirichlet_kl(q, p).numpy())\n",
        "p2 = tfd.Dirichlet(jnp.ones(2))\n",
        "q2 = tfd.Dirichlet(2 * jnp.ones(2))\n",
        "print(\"tfp kl: \", q2.kl_divergence(p2))\n"
      ],
      "metadata": {
        "id": "5h2ORgr1FOlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bCID4jque3N"
      },
      "source": [
        "### Problem 4a: Implement CAVI updates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-TIiIlxue3O"
      },
      "outputs": [],
      "source": [
        "def cavi_step_z(data, q_pi, q_theta):\n",
        "    \"\"\"\n",
        "    Perform a CAVI update of q(z) given the data, q(\\pi), and q(\\theta)\n",
        "\n",
        "    Args:\n",
        "        data: (N, D) tensor where each row is a data point\n",
        "        q_pi: td.Dirichlet posterior distribution over the mixture weights\n",
        "        q_theta: td.MultivariateNormal posterior distribution over the set of\n",
        "            mixture means. I.e. `q_theta.mean.shape == (K, D)` and \n",
        "            `q_theta.covariance_matrix.shape == (K, D, D)` where K is the \n",
        "            number of mixture components.\n",
        "\n",
        "    Returns:\n",
        "        q_z: a Categorical distribution over a batch of N mixture assignments.\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # Your code here\n",
        "    ##\n",
        "    return Categorical(logits=...)\n",
        "    \n",
        "\n",
        "def cavi_step_pi(q_z, alpha):\n",
        "    \"\"\"\n",
        "    Performa CAVI update of q(\\pi) given q(z) and alpha.\n",
        "\n",
        "    Args:\n",
        "        q_z: Categorical posterior distribution over mixture assignments\n",
        "        alpha: scalar (or shape (K,) tensor of) prior concentration(s)\n",
        "\n",
        "    Returns:\n",
        "        q_pi: Dirichlet posterior distribution over mixture weights\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # Your code here\n",
        "    ##\n",
        "    return Dirichlet(...)\n",
        "\n",
        "\n",
        "def cavi_step_theta(data, q_z, beta):\n",
        "    \"\"\"\n",
        "    Performa CAVI update of q(\\pi) given q(z) and alpha.\n",
        "\n",
        "    Args:\n",
        "        q_z: Categorical posterior distribution over mixture assignments\n",
        "        alpha: scalar (or shape (K,) tensor of) prior concentration(s)\n",
        "\n",
        "    Returns:\n",
        "        q_pi: Dirichlet posterior distribution over mixture weights\n",
        "    \"\"\"\n",
        "    ###\n",
        "    # Your code here\n",
        "    ##\n",
        "    return MultivariateNormal(..., ...)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 4b: Implement the ELBO"
      ],
      "metadata": {
        "id": "R6XdL-icdVDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def elbo(data, q_z, q_pi, q_theta, alpha, beta):\n",
        "    \"\"\"Compute the evidence lower bound.\n",
        "    \"\"\"\n",
        "    E_z = q_z.probs\n",
        "    N, D = data.shape\n",
        "    K = q_pi.concentration.shape[-1]\n",
        "\n",
        "    ###\n",
        "    # Your code here\n",
        "    ##\n",
        "    \n",
        "    return elbo"
      ],
      "metadata": {
        "id": "N_R8sA6wdc6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Put it all together"
      ],
      "metadata": {
        "id": "tF33DQvodU9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cavi(data, \n",
        "         num_components=2, \n",
        "         num_iter=100, \n",
        "         tol=1e-5, \n",
        "         alpha=20.0, \n",
        "         beta=1.0,\n",
        "         seed=305 + ord('c'),\n",
        "        ):\n",
        "    \"\"\"Run coordinate ascent VI for the Gaussian Mixture Model.\n",
        "    \n",
        "    \"\"\"\n",
        "    data = data.type(torch.float)\n",
        "    N, D = data.shape\n",
        "    K = num_components      # short hand\n",
        "    \n",
        "    # Initialize the clusters randomly\n",
        "    torch.manual_seed(seed)\n",
        "    clusters = Categorical(logits=torch.zeros(K)).sample((N,))\n",
        "    q_pi = Dirichlet(alpha * torch.ones(K))\n",
        "    q_theta = MultivariateNormal(\n",
        "        torch.row_stack([data[clusters == k].mean(axis=0) for k in range(K)]),\n",
        "        torch.eye(D).repeat(K, 1, 1))\n",
        "    q_z = Categorical(logits=torch.zeros((N, K)))\n",
        "    \n",
        "    # Run CAVI\n",
        "    elbos = [elbo(data, q_z, q_pi, q_theta, alpha, beta)]\n",
        "    for itr in trange(num_iter):\n",
        "        # Update variational factors one at a time\n",
        "        q_z = cavi_step_z(data, q_pi, q_theta)\n",
        "        q_pi = cavi_step_pi(q_z, alpha)\n",
        "        q_theta = cavi_step_theta(data, q_z, beta)\n",
        "        \n",
        "        # Compute the ELBO\n",
        "        elbos.append(elbo(data, q_z, q_pi, q_theta, alpha, beta))\n",
        "        \n",
        "        # Check for convergence\n",
        "        if elbos[-1] - elbos[-2] < -1e-4:\n",
        "            raise Exception(\"ELBO is going down!\")\n",
        "        elif elbos[-1] - elbos[-2] < tol:\n",
        "            print(\"Converged!\")\n",
        "            break\n",
        "        \n",
        "    return torch.tensor(elbos), (q_z, q_pi, q_theta)\n",
        "        "
      ],
      "metadata": {
        "id": "yFrabsbqdSgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test on synthetic data"
      ],
      "metadata": {
        "id": "Ga2QoF_k89w9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data points   \n",
        "torch.manual_seed(305 + ord('c'))\n",
        "K = 3\n",
        "D = 2\n",
        "N = 60\n",
        "true_thetas = torch.Tensor([[2, 2], [5, 5], [8, 8]])\n",
        "X = MultivariateNormal(\n",
        "    true_thetas, torch.eye(D)).sample((N // K,)).reshape(-1, D)\n",
        "\n",
        "# Run the CAVI algorithm\n",
        "elbos, (q_z, q_pi, q_theta) = \\\n",
        "    cavi(X, \n",
        "         num_components=K, \n",
        "         alpha=torch.ones(K),\n",
        "         beta=1.0)\n",
        "    \n",
        "# Print the results  \n",
        "for k in range(K):\n",
        "    print(\"Cluster \", k, \":\")\n",
        "    print(\"\\t E[mu_k]:   \", q_theta.mean[k,:])\n",
        "    print(\"\\t Cov[mu_k]: \", q_theta.covariance_matrix[k,:,:])\n",
        "    print(\"\\t E[pi_k]:   \", q_pi.mean[k])\n",
        "    print(\"\")\n",
        "\n",
        "# Plot the log probabilities over EM iterations\n",
        "plt.figure()\n",
        "plt.plot(elbos[1:])\n",
        "plt.xlabel(\"CAVI iteration\")\n",
        "plt.ylabel(\"ELBO\")\n",
        "\n",
        "# create a second figure to plot the clustered data\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "# plot scatter \n",
        "colors = [\"red\", \"blue\", \"green\"]\n",
        "z_hat = torch.argmax(q_z.probs, 1)\n",
        "\n",
        "for k in range(K):\n",
        "    ax.plot(X[z_hat==k, 0], X[z_hat==k, 1], c=colors[k], ls='', marker='.')\n",
        "    ax.plot(q_theta.mean[k, 0], q_theta.mean[k, 1], c=colors[k], \n",
        "            mec='k', marker='o', ms=8)\n"
      ],
      "metadata": {
        "id": "vWW182t69ZlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "Truth be told, I spent about an hour this morning debugging my CAVI implementation until I found a ridiculously simple bug! The code was still returning sensible results, but the ELBO was going down. I thought for sure it was a bug in my ELBO calculation (because that's where it usually is), but this time it was in my update for $q(\\theta_k)$. In the end, I guess checking if the ELBO decreased was a useful debugging tool!"
      ],
      "metadata": {
        "id": "9cDAAFhl7bto"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Lecture 10: CAVI for Gaussian Mixture Models",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}